{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660d7b34",
   "metadata": {},
   "source": [
    "## Configuration v√† Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da45f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Training Configuration:\n",
      "  Force Retrain: True\n",
      "  Skip on Error: True\n",
      "  Train XGBoost: True\n",
      "  Train LSTM: True\n",
      "  Configurations: ['7n_1n', '30n_1n', '30n_7n', '30n_30n', '90n_7n', '90n_30n']\n",
      "  Total Models to Train: 12\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# MAIN CONFIGURATION\n",
    "# ===============================================\n",
    "FORCE_RETRAIN = True  # Set True ƒë·ªÉ train l·∫°i models ƒë√£ c√≥\n",
    "SKIP_ON_ERROR = True   # Set True ƒë·ªÉ skip configuration b·ªã l·ªói v√† ti·∫øp t·ª•c\n",
    "SAVE_DETAILED_LOGS = True  # Set True ƒë·ªÉ l∆∞u logs chi ti·∫øt\n",
    "\n",
    "# Model selection (c√≥ th·ªÉ ch·ªçn train m·ªôt ph·∫ßn)\n",
    "TRAIN_XGBOOST = True\n",
    "TRAIN_LSTM = True\n",
    "\n",
    "# Configurations to train (c√≥ th·ªÉ customize)\n",
    "CONFIGS_TO_TRAIN = ['7n_1n', '30n_1n', '30n_7n', '30n_30n', '90n_7n', '90n_30n']\n",
    "# CONFIGS_TO_TRAIN = ['7n_1n', '30n_1n']  # Uncomment ƒë·ªÉ test v·ªõi configs nh·ªè\n",
    "\n",
    "print(f\"üéØ Training Configuration:\")\n",
    "print(f\"  Force Retrain: {FORCE_RETRAIN}\")\n",
    "print(f\"  Skip on Error: {SKIP_ON_ERROR}\")\n",
    "print(f\"  Train XGBoost: {TRAIN_XGBOOST}\")\n",
    "print(f\"  Train LSTM: {TRAIN_LSTM}\")\n",
    "print(f\"  Configurations: {CONFIGS_TO_TRAIN}\")\n",
    "print(f\"  Total Models to Train: {len(CONFIGS_TO_TRAIN) * (int(TRAIN_XGBOOST) + int(TRAIN_LSTM))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293e87d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 23:03:55.746341: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-03 23:03:55.747196: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-03 23:03:55.855961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-03 23:03:57.327618: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-03 23:03:57.328500: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TensorFlow 2.20.0 available\n",
      "üíª Using CPU for training\n",
      "‚úÖ LSTM trainer imported successfully\n",
      "‚úÖ Libraries imported successfully\n",
      "üé≤ Random seed: 28112001\n",
      "üìä Available experiments: ['7n_1n', '30n_1n', '30n_7n', '30n_30n', '90n_7n', '90n_30n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759507438.258922   70494 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1759507438.263447   70494 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Check TensorFlow availability\n",
    "TF_AVAILABLE = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TF_AVAILABLE = True\n",
    "    print(f\"‚úÖ TensorFlow {tf.__version__} available\")\n",
    "    \n",
    "    # Check for GPU\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(f\"üöÄ GPU acceleration available\")\n",
    "    else:\n",
    "        print(f\"üíª Using CPU for training\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå TensorFlow not available: {e}\")\n",
    "    print(f\"‚ö†Ô∏è LSTM training will fail\")\n",
    "\n",
    "# Import trainers\n",
    "from xgboost_trainer import train_xgboost_model\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    from lstm_trainer import train_lstm_model\n",
    "    print(f\"‚úÖ LSTM trainer imported successfully\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è LSTM trainer not available - TensorFlow missing\")\n",
    "\n",
    "from config import EXPERIMENTS, XGBOOST_PARAMS, LSTM_PARAMS, RANDOM_SEED\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üé≤ Random seed: {RANDOM_SEED}\")\n",
    "print(f\"üìä Available experiments: {list(EXPERIMENTS.keys())}\")\n",
    "\n",
    "# Update training flags based on availability\n",
    "if not TF_AVAILABLE and TRAIN_LSTM:\n",
    "    print(f\"‚ö†Ô∏è Disabling LSTM training due to missing TensorFlow\")\n",
    "    TRAIN_LSTM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d261e2e",
   "metadata": {},
   "source": [
    "## Training Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6a003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training tracker initialized\n"
     ]
    }
   ],
   "source": [
    "class TrainingTracker:\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.results = []\n",
    "        self.errors = []\n",
    "        self.current_step = 0\n",
    "        self.total_steps = 0\n",
    "        \n",
    "    def set_total_steps(self, total):\n",
    "        self.total_steps = total\n",
    "        \n",
    "    def start_training(self, config_name, model_type):\n",
    "        self.current_step += 1\n",
    "        self.current_config = config_name\n",
    "        self.current_model = model_type\n",
    "        self.step_start_time = time.time()\n",
    "        \n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üöÄ STEP {self.current_step}/{self.total_steps}: {config_name.upper()} - {model_type.upper()}\")\n",
    "        print(f\"‚è±Ô∏è Elapsed: {elapsed/60:.1f}min | ETA: {elapsed/self.current_step*(self.total_steps-self.current_step)/60:.1f}min\")\n",
    "        print(f\"üìä Experiment: {EXPERIMENTS[config_name]['description']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "    def end_training(self, success=True, error_msg=None, results=None):\n",
    "        step_time = time.time() - self.step_start_time\n",
    "        \n",
    "        result = {\n",
    "            'config': self.current_config,\n",
    "            'model_type': self.current_model,\n",
    "            'success': success,\n",
    "            'training_time': step_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if success and results:\n",
    "            result.update({\n",
    "                'test_rmse': results.get('test_metrics', {}).get('RMSE', None),\n",
    "                'test_r2': results.get('test_metrics', {}).get('R2', None),\n",
    "                'best_params': results.get('best_params', {})\n",
    "            })\n",
    "            print(f\"‚úÖ SUCCESS: {self.current_config} {self.current_model} - {step_time/60:.1f}min\")\n",
    "            if results.get('test_metrics'):\n",
    "                print(f\"   RMSE: {results['test_metrics']['RMSE']:.4f} | R¬≤: {results['test_metrics']['R2']:.4f}\")\n",
    "        else:\n",
    "            result['error'] = error_msg\n",
    "            self.errors.append(result)\n",
    "            print(f\"‚ùå FAILED: {self.current_config} {self.current_model} - {step_time/60:.1f}min\")\n",
    "            print(f\"   Error: {error_msg}\")\n",
    "            \n",
    "        self.results.append(result)\n",
    "        \n",
    "    def get_summary(self):\n",
    "        total_time = time.time() - self.start_time\n",
    "        successful = [r for r in self.results if r['success']]\n",
    "        failed = [r for r in self.results if not r['success']]\n",
    "        \n",
    "        return {\n",
    "            'total_time': total_time,\n",
    "            'total_models': len(self.results),\n",
    "            'successful': len(successful),\n",
    "            'failed': len(failed),\n",
    "            'success_rate': len(successful) / len(self.results) * 100 if self.results else 0,\n",
    "            'results': self.results,\n",
    "            'errors': self.errors\n",
    "        }\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = TrainingTracker()\n",
    "print(\"‚úÖ Training tracker initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d5e92",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab52b2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking data availability...\n",
      "‚úÖ All required data is available\n",
      "\n",
      "üîç Checking existing models...\n",
      "‚ö†Ô∏è Found 3 existing models:\n",
      "   - 7n_1n_xgb\n",
      "   - 7n_1n_lstm\n",
      "   - 30n_1n_xgb\n",
      "\n",
      "üîÑ These will be retrained (FORCE_RETRAIN=True)\n",
      "\n",
      "üéØ Ready to train models!\n"
     ]
    }
   ],
   "source": [
    "def validate_data_availability():\n",
    "    \"\"\"Ki·ªÉm tra data ƒë√£ s·∫µn s√†ng cho t·∫•t c·∫£ configurations\"\"\"\n",
    "    data_folder = '../data'\n",
    "    missing_data = []\n",
    "    \n",
    "    print(\"üîç Checking data availability...\")\n",
    "    \n",
    "    for config in CONFIGS_TO_TRAIN:\n",
    "        # Check XGBoost data\n",
    "        if TRAIN_XGBOOST:\n",
    "            xgb_folder = f\"{data_folder}/{config}_xgb\"\n",
    "            if not os.path.exists(xgb_folder):\n",
    "                missing_data.append(f\"{config}_xgb\")\n",
    "            else:\n",
    "                required_files = ['X_train.csv', 'X_test.csv', 'y_train.csv', 'y_test.csv', 'metadata.json']\n",
    "                for file in required_files:\n",
    "                    if not os.path.exists(f\"{xgb_folder}/{file}\"):\n",
    "                        missing_data.append(f\"{config}_xgb/{file}\")\n",
    "        \n",
    "        # Check LSTM data\n",
    "        if TRAIN_LSTM:\n",
    "            lstm_folder = f\"{data_folder}/{config}_lstm\"\n",
    "            if not os.path.exists(lstm_folder):\n",
    "                missing_data.append(f\"{config}_lstm\")\n",
    "            else:\n",
    "                required_files = ['X_train.npy', 'X_test.npy', 'y_train.npy', 'y_test.npy', 'metadata.json']\n",
    "                for file in required_files:\n",
    "                    if not os.path.exists(f\"{lstm_folder}/{file}\"):\n",
    "                        missing_data.append(f\"{config}_lstm/{file}\")\n",
    "    \n",
    "    if missing_data:\n",
    "        print(\"‚ùå Missing data detected:\")\n",
    "        for item in missing_data:\n",
    "            print(f\"   - {item}\")\n",
    "        print(\"\\nüí° Please run notebook 02_feature_engineering.ipynb first\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"‚úÖ All required data is available\")\n",
    "        return True\n",
    "\n",
    "def check_existing_models():\n",
    "    \"\"\"Ki·ªÉm tra models ƒë√£ train\"\"\"\n",
    "    models_folder = '../models'\n",
    "    existing_models = []\n",
    "    \n",
    "    print(\"\\nüîç Checking existing models...\")\n",
    "    \n",
    "    for config in CONFIGS_TO_TRAIN:\n",
    "        if TRAIN_XGBOOST:\n",
    "            xgb_model = f\"{models_folder}/{config}_xgb/best_model.pkl\"\n",
    "            if os.path.exists(xgb_model):\n",
    "                existing_models.append(f\"{config}_xgb\")\n",
    "                \n",
    "        if TRAIN_LSTM:\n",
    "            lstm_model = f\"{models_folder}/{config}_lstm/best_model.h5\"\n",
    "            if os.path.exists(lstm_model):\n",
    "                existing_models.append(f\"{config}_lstm\")\n",
    "    \n",
    "    if existing_models:\n",
    "        print(f\"‚ö†Ô∏è Found {len(existing_models)} existing models:\")\n",
    "        for model in existing_models:\n",
    "            print(f\"   - {model}\")\n",
    "        if not FORCE_RETRAIN:\n",
    "            print(f\"\\nüí° These will be skipped (set FORCE_RETRAIN=True to retrain)\")\n",
    "        else:\n",
    "            print(f\"\\nüîÑ These will be retrained (FORCE_RETRAIN=True)\")\n",
    "    else:\n",
    "        print(\"‚úÖ No existing models found, will train all\")\n",
    "        \n",
    "    return existing_models\n",
    "\n",
    "# Run validation\n",
    "data_ready = validate_data_availability()\n",
    "existing_models = check_existing_models()\n",
    "\n",
    "if not data_ready:\n",
    "    raise RuntimeError(\"Data not ready for training\")\n",
    "    \n",
    "print(f\"\\nüéØ Ready to train models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac20ad7",
   "metadata": {},
   "source": [
    "## Training Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9356168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã TRAINING PLAN:\n",
      "   Total models to train: 12\n",
      "\n",
      "üìù Training sequence:\n",
      "    1. 7N_1N    XGBOOST  (7 days input to predict water level at day 8 (not mean of days 8-14))\n",
      "    2. 7N_1N    LSTM     (7 days input to predict water level at day 8 (not mean of days 8-14))\n",
      "    3. 30N_1N   XGBOOST  (30 days input to predict water level at day 31)\n",
      "    4. 30N_1N   LSTM     (30 days input to predict water level at day 31)\n",
      "    5. 30N_7N   XGBOOST  (30 days input to predict water level at day 37 (not mean of days 31-37))\n",
      "    6. 30N_7N   LSTM     (30 days input to predict water level at day 37 (not mean of days 31-37))\n",
      "    7. 30N_30N  XGBOOST  (30 days input to predict water level at day 60)\n",
      "    8. 30N_30N  LSTM     (30 days input to predict water level at day 60)\n",
      "    9. 90N_7N   XGBOOST  (90 days input to predict water level at day 97)\n",
      "   10. 90N_7N   LSTM     (90 days input to predict water level at day 97)\n",
      "   11. 90N_30N  XGBOOST  (90 days input to predict water level at day 120)\n",
      "   12. 90N_30N  LSTM     (90 days input to predict water level at day 120)\n",
      "\n",
      "üöÄ Ready to start training 12 models!\n"
     ]
    }
   ],
   "source": [
    "def create_training_plan():\n",
    "    \"\"\"T·∫°o k·∫ø ho·∫°ch training\"\"\"\n",
    "    plan = []\n",
    "    \n",
    "    for config in CONFIGS_TO_TRAIN:\n",
    "        if TRAIN_XGBOOST:\n",
    "            model_exists = f\"{config}_xgb\" in [m.replace('../models/', '').replace('/best_model.pkl', '') for m in existing_models]\n",
    "            if not model_exists or FORCE_RETRAIN:\n",
    "                plan.append((config, 'xgboost'))\n",
    "                \n",
    "        if TRAIN_LSTM and TF_AVAILABLE:\n",
    "            model_exists = f\"{config}_lstm\" in [m.replace('../models/', '').replace('/best_model.keras', '') for m in existing_models]\n",
    "            if not model_exists or FORCE_RETRAIN:\n",
    "                plan.append((config, 'lstm'))\n",
    "        elif TRAIN_LSTM and not TF_AVAILABLE:\n",
    "            print(f\"‚ö†Ô∏è Skipping LSTM for {config} - TensorFlow not available\")\n",
    "    \n",
    "    return plan\n",
    "\n",
    "# Create plan\n",
    "training_plan = create_training_plan()\n",
    "\n",
    "print(f\"üìã TRAINING PLAN:\")\n",
    "print(f\"   Total models to train: {len(training_plan)}\")\n",
    "print(f\"\\nüìù Training sequence:\")\n",
    "for i, (config, model_type) in enumerate(training_plan, 1):\n",
    "    exp = EXPERIMENTS[config]\n",
    "    print(f\"   {i:2d}. {config.upper():<8} {model_type.upper():<8} ({exp['description']})\")\n",
    "\n",
    "# Set up tracker\n",
    "tracker.set_total_steps(len(training_plan))\n",
    "\n",
    "if len(training_plan) == 0:\n",
    "    print(\"\\n‚úÖ All models already trained and FORCE_RETRAIN=False\")\n",
    "    print(\"üí° Set FORCE_RETRAIN=True if you want to retrain existing models\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Ready to start training {len(training_plan)} models!\")\n",
    "\n",
    "# Additional warnings\n",
    "if TRAIN_LSTM and not TF_AVAILABLE:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: LSTM training requested but TensorFlow not available\")\n",
    "    print(f\"   Please install TensorFlow: pip install tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf6d34",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff7fb125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Progress: 12/12 models trained\n",
      "   üìä Starting LSTM training for 90n_30n\n",
      "   üìÅ Data folder: ../data/90n_30n_lstm\n",
      "   üéØ Parameters: {'units': [32, 64], 'n_layers': [1, 2], 'dropout': [0.2, 0.5], 'batch_size': [32], 'epochs': [100], 'patience': [10]}\n",
      "Loaded data for 90n_30n:\n",
      "  X_train: (7328, 90, 6)\n",
      "  X_test: (1743, 90, 6)\n",
      "  y_train: (7328,) (scaled)\n",
      "  y_test: (1743,) (scaled)\n",
      "  Target scaler fitted: mean=0.8946, std=0.5896\n",
      "\\nStarting grid search for 90n_30n...\n",
      "Parameter combinations: 8\n",
      "\\nTesting combination 1: {'batch_size': 32, 'dropout': 0.2, 'n_layers': 1, 'units': 32}\n",
      "  Val Loss: 0.182778, Epochs: 73\n",
      "  >>> New best model!\n",
      "\\nTesting combination 2: {'batch_size': 32, 'dropout': 0.2, 'n_layers': 1, 'units': 64}\n",
      "  Val Loss: 0.180188, Epochs: 38\n",
      "  >>> New best model!\n",
      "\\nTesting combination 3: {'batch_size': 32, 'dropout': 0.2, 'n_layers': 2, 'units': 32}\n",
      "  Val Loss: 0.164282, Epochs: 77\n",
      "  >>> New best model!\n",
      "\\nTesting combination 4: {'batch_size': 32, 'dropout': 0.2, 'n_layers': 2, 'units': 64}\n",
      "  Val Loss: 0.154797, Epochs: 40\n",
      "  >>> New best model!\n",
      "\\nTesting combination 5: {'batch_size': 32, 'dropout': 0.5, 'n_layers': 1, 'units': 32}\n",
      "  Val Loss: 0.329715, Epochs: 50\n",
      "\\nTesting combination 6: {'batch_size': 32, 'dropout': 0.5, 'n_layers': 1, 'units': 64}\n",
      "  Val Loss: 0.165842, Epochs: 54\n",
      "\\nTesting combination 7: {'batch_size': 32, 'dropout': 0.5, 'n_layers': 2, 'units': 32}\n",
      "  Val Loss: 0.192518, Epochs: 100\n",
      "\\nTesting combination 8: {'batch_size': 32, 'dropout': 0.5, 'n_layers': 2, 'units': 64}\n",
      "  Val Loss: 0.168023, Epochs: 61\n",
      "\\nBest parameters: {'batch_size': 32, 'dropout': 0.2, 'n_layers': 2, 'units': 64}\n",
      "Best validation loss: 0.154797\n",
      "\\nTraining final model with best parameters...\n",
      "Epoch 1/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 103ms/step - loss: 0.9038 - mae: 0.7597 - val_loss: 0.8365 - val_mae: 0.7410\n",
      "Epoch 2/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - loss: 0.8648 - mae: 0.7378 - val_loss: 0.8115 - val_mae: 0.7239\n",
      "Epoch 3/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 103ms/step - loss: 0.8194 - mae: 0.7151 - val_loss: 0.7458 - val_mae: 0.6963\n",
      "Epoch 4/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - loss: 0.7401 - mae: 0.6678 - val_loss: 0.6281 - val_mae: 0.6318\n",
      "Epoch 5/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 105ms/step - loss: 0.6554 - mae: 0.6215 - val_loss: 0.5177 - val_mae: 0.5694\n",
      "Epoch 6/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 102ms/step - loss: 0.5982 - mae: 0.5908 - val_loss: 0.4511 - val_mae: 0.5316\n",
      "Epoch 7/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - loss: 0.5219 - mae: 0.5446 - val_loss: 0.3614 - val_mae: 0.4740\n",
      "Epoch 8/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 105ms/step - loss: 0.4578 - mae: 0.5080 - val_loss: 0.3160 - val_mae: 0.4374\n",
      "Epoch 9/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 102ms/step - loss: 0.4125 - mae: 0.4766 - val_loss: 0.2864 - val_mae: 0.4056\n",
      "Epoch 10/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 105ms/step - loss: 0.3865 - mae: 0.4550 - val_loss: 0.2467 - val_mae: 0.3745\n",
      "Epoch 11/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 101ms/step - loss: 0.3618 - mae: 0.4382 - val_loss: 0.2241 - val_mae: 0.3556\n",
      "Epoch 12/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - loss: 0.3361 - mae: 0.4159 - val_loss: 0.2187 - val_mae: 0.3476\n",
      "Epoch 13/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 104ms/step - loss: 0.3160 - mae: 0.4015 - val_loss: 0.1919 - val_mae: 0.3210\n",
      "Epoch 14/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - loss: 0.3000 - mae: 0.3850 - val_loss: 0.1854 - val_mae: 0.3116\n",
      "Epoch 15/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 105ms/step - loss: 0.2897 - mae: 0.3788 - val_loss: 0.1950 - val_mae: 0.3193\n",
      "Epoch 16/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - loss: 0.2789 - mae: 0.3648 - val_loss: 0.1887 - val_mae: 0.3023\n",
      "Epoch 17/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 101ms/step - loss: 0.2682 - mae: 0.3598 - val_loss: 0.2117 - val_mae: 0.3222\n",
      "Epoch 18/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 104ms/step - loss: 0.2637 - mae: 0.3545 - val_loss: 0.1856 - val_mae: 0.2969\n",
      "Epoch 19/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - loss: 0.2553 - mae: 0.3472 - val_loss: 0.1942 - val_mae: 0.3038\n",
      "Epoch 20/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 109ms/step - loss: 0.2498 - mae: 0.3419 - val_loss: 0.1731 - val_mae: 0.2885\n",
      "Epoch 21/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - loss: 0.2440 - mae: 0.3369 - val_loss: 0.1820 - val_mae: 0.3025\n",
      "Epoch 22/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - loss: 0.2400 - mae: 0.3352 - val_loss: 0.1779 - val_mae: 0.2920\n",
      "Epoch 23/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 104ms/step - loss: 0.2402 - mae: 0.3315 - val_loss: 0.1725 - val_mae: 0.2775\n",
      "Epoch 24/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 99ms/step - loss: 0.2324 - mae: 0.3261 - val_loss: 0.1724 - val_mae: 0.2797\n",
      "Epoch 25/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 105ms/step - loss: 0.2298 - mae: 0.3224 - val_loss: 0.1736 - val_mae: 0.2785\n",
      "Epoch 26/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 103ms/step - loss: 0.2195 - mae: 0.3169 - val_loss: 0.1778 - val_mae: 0.2844\n",
      "Epoch 27/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 101ms/step - loss: 0.2199 - mae: 0.3165 - val_loss: 0.1761 - val_mae: 0.2754\n",
      "Epoch 28/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 104ms/step - loss: 0.2179 - mae: 0.3115 - val_loss: 0.1746 - val_mae: 0.2759\n",
      "Epoch 29/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 101ms/step - loss: 0.2205 - mae: 0.3150 - val_loss: 0.1829 - val_mae: 0.2821\n",
      "Epoch 30/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 104ms/step - loss: 0.2135 - mae: 0.3110 - val_loss: 0.1837 - val_mae: 0.2861\n",
      "Epoch 31/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 101ms/step - loss: 0.2114 - mae: 0.3078 - val_loss: 0.1747 - val_mae: 0.2767\n",
      "Epoch 32/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 98ms/step - loss: 0.2101 - mae: 0.3073 - val_loss: 0.1758 - val_mae: 0.2718\n",
      "Epoch 33/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 104ms/step - loss: 0.2120 - mae: 0.3104 - val_loss: 0.1758 - val_mae: 0.2734\n",
      "Epoch 34/100\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 100ms/step - loss: 0.2086 - mae: 0.3014 - val_loss: 0.1885 - val_mae: 0.2959\n",
      "Epoch 34: early stopping\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Final model trained successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== MODEL EVALUATION (Original Scale) ===\n",
      "Training metrics:\n",
      "  MAE: 0.151183\n",
      "  MSE: 0.056175\n",
      "  RMSE: 0.237012\n",
      "  R2: 0.838400\n",
      "\\nTest metrics:\n",
      "  MAE: 0.348969\n",
      "  MSE: 0.267996\n",
      "  RMSE: 0.517683\n",
      "  R2: 0.178812\n",
      "\\nResults saved to ../models/90n_30n_lstm/\n",
      "  - best_model.h5\n",
      "  - grid_search_results_full.csv (all combinations)\n",
      "  - grid_analysis.csv (analysis)\n",
      "  - training_history.csv\n",
      "  - results.json (summary)\n",
      "\\n‚úÖ LSTM training completed for 90n_30n\n",
      "   üìÑ Loading results from: ../models/90n_30n_lstm/results.json\n",
      "‚úÖ SUCCESS: 90n_30n lstm - 104.1min\n",
      "   RMSE: 0.5177 | R¬≤: 0.1788\n",
      "\n",
      "üèÅ Training execution completed!\n",
      "‚è∞ End time: 2025-10-04 04:50:35\n"
     ]
    }
   ],
   "source": [
    "def train_single_model(config_name, model_type):\n",
    "    \"\"\"Train m·ªôt model cho config v√† type c·ª• th·ªÉ\"\"\"\n",
    "    import os  # Move os import to the top\n",
    "    \n",
    "    try:\n",
    "        if model_type == 'xgboost':\n",
    "            # Suppress verbose output for XGBoost to reduce clutter\n",
    "            old_stdout = os.dup(1)\n",
    "            os.close(1)\n",
    "            os.open(os.devnull, os.O_RDWR)\n",
    "            \n",
    "            try:\n",
    "                trainer = train_xgboost_model(\n",
    "                    config_name=config_name,\n",
    "                    param_grid=XGBOOST_PARAMS,\n",
    "                    data_folder='../data',\n",
    "                    models_folder='../models',\n",
    "                    cv_folds=3\n",
    "                )\n",
    "            finally:\n",
    "                # Restore stdout\n",
    "                os.dup2(old_stdout, 1)\n",
    "                os.close(old_stdout)\n",
    "            \n",
    "            # Load results\n",
    "            results_file = f\"../models/{config_name}_xgb/results.json\"\n",
    "            with open(results_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "                \n",
    "        elif model_type == 'lstm':\n",
    "            if not TF_AVAILABLE:\n",
    "                raise RuntimeError(\"TensorFlow not available for LSTM training\")\n",
    "                \n",
    "            print(f\"   üìä Starting LSTM training for {config_name}\")\n",
    "            print(f\"   üìÅ Data folder: ../data/{config_name}_lstm\")\n",
    "            print(f\"   üéØ Parameters: {LSTM_PARAMS}\")\n",
    "            \n",
    "            # Check if LSTM data exists\n",
    "            lstm_data_folder = f\"../data/{config_name}_lstm\"\n",
    "            required_files = ['X_train.npy', 'X_test.npy', 'y_train.npy', 'y_test.npy']\n",
    "            for file in required_files:\n",
    "                file_path = f\"{lstm_data_folder}/{file}\"\n",
    "                if not os.path.exists(file_path):\n",
    "                    raise FileNotFoundError(f\"Required LSTM data file not found: {file_path}\")\n",
    "            \n",
    "            # Extract epochs and patience from param_grid\n",
    "            lstm_param_grid = LSTM_PARAMS.copy()\n",
    "            epochs = lstm_param_grid.pop('epochs', [100])[0] if isinstance(lstm_param_grid.get('epochs', [100]), list) else lstm_param_grid.pop('epochs', 100)\n",
    "            patience = lstm_param_grid.pop('patience', [5])[0] if isinstance(lstm_param_grid.get('patience', [5]), list) else lstm_param_grid.pop('patience', 5)\n",
    "                    \n",
    "            trainer = train_lstm_model(\n",
    "                config_name=config_name,\n",
    "                param_grid=lstm_param_grid,  # Pass grid without epochs/patience\n",
    "                data_folder='../data',\n",
    "                models_folder='../models',\n",
    "                epochs=epochs,  # Pass as separate parameter\n",
    "                patience=patience,  # Pass as separate parameter\n",
    "                verbose=0  # Reduce verbosity\n",
    "            )\n",
    "            \n",
    "            # Load results\n",
    "            results_file = f\"../models/{config_name}_lstm/results.json\"\n",
    "            print(f\"   üìÑ Loading results from: {results_file}\")\n",
    "            \n",
    "            if not os.path.exists(results_file):\n",
    "                raise FileNotFoundError(f\"Results file not found: {results_file}\")\n",
    "                \n",
    "            with open(results_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "        \n",
    "        return True, results, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_msg = str(e)\n",
    "        traceback_msg = traceback.format_exc()\n",
    "        print(f\"\\nüí• Error training {config_name} {model_type}:\")\n",
    "        print(f\"   Error: {error_msg}\")\n",
    "        # Only print traceback for debugging if needed\n",
    "        if \"TensorFlow not available\" not in error_msg and \"Results file not found\" not in error_msg:\n",
    "            print(f\"   Traceback: {traceback_msg}\")\n",
    "        return False, None, error_msg\n",
    "\n",
    "# Execute training plan\n",
    "if len(training_plan) > 0:\n",
    "    print(f\"\\nüöÄ Starting training execution...\")\n",
    "    print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"üìä Total models to train: {len(training_plan)}\")\n",
    "    \n",
    "    for config_name, model_type in training_plan:\n",
    "        tracker.start_training(config_name, model_type)\n",
    "        \n",
    "        # Clear output periodically to prevent truncation\n",
    "        if tracker.current_step % 3 == 0:\n",
    "            from IPython.display import clear_output\n",
    "            clear_output(wait=True)\n",
    "            print(f\"üîÑ Progress: {tracker.current_step}/{len(training_plan)} models trained\")\n",
    "        \n",
    "        success, results, error_msg = train_single_model(config_name, model_type)\n",
    "        tracker.end_training(success, error_msg, results)\n",
    "        \n",
    "        # Skip on error if configured\n",
    "        if not success and not SKIP_ON_ERROR:\n",
    "            print(f\"\\nüõë Training stopped due to error (SKIP_ON_ERROR=False)\")\n",
    "            break\n",
    "            \n",
    "        # Brief pause between models\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\nüèÅ Training execution completed!\")\n",
    "    print(f\"‚è∞ End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "else:\n",
    "    print(f\"\\n‚è≠Ô∏è No models to train (all exist and FORCE_RETRAIN=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be14f1d",
   "metadata": {},
   "source": [
    "## Training Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a315be84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä TRAINING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚è±Ô∏è Execution Time:\n",
      "   Total time: 5.8h 47min\n",
      "   Average per model: 28.9min\n",
      "\n",
      "üìà Success Rate:\n",
      "   Total models: 12\n",
      "   Successful: 12 ‚úÖ\n",
      "   Failed: 0 ‚ùå\n",
      "   Success rate: 100.0%\n",
      "\n",
      "üìã Detailed Results:\n",
      " config model_type  success training_time test_rmse test_r2\n",
      "  7n_1n    xgboost     True        1.1min    0.2841  0.7598\n",
      "  7n_1n       lstm     True        7.4min    0.5451  0.1162\n",
      " 30n_1n    xgboost     True        4.6min    0.2126  0.8656\n",
      " 30n_1n       lstm     True       33.8min    0.5901 -0.0353\n",
      " 30n_7n    xgboost     True        4.5min    0.2535  0.8090\n",
      " 30n_7n       lstm     True       34.4min    0.5110  0.2237\n",
      "30n_30n    xgboost     True        4.6min    0.3692  0.5939\n",
      "30n_30n       lstm     True       36.5min    0.5541  0.0853\n",
      " 90n_7n    xgboost     True       13.1min    0.2291  0.8400\n",
      " 90n_7n       lstm     True       88.6min    0.3511  0.6244\n",
      "90n_30n    xgboost     True       13.7min    0.3073  0.7106\n",
      "90n_30n       lstm     True      104.1min    0.5177  0.1788\n",
      "\n",
      "üíæ Summary saved to: ../models/training_summary_20251004_045035.json\n",
      "\n",
      "üéâ All done! Check individual model folders in ../models/ for detailed results.\n"
     ]
    }
   ],
   "source": [
    "# Get comprehensive summary\n",
    "summary = tracker.get_summary()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"üìä TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Execution Time:\")\n",
    "print(f\"   Total time: {summary['total_time']/3600:.1f}h {(summary['total_time']%3600)/60:.0f}min\")\n",
    "print(f\"   Average per model: {summary['total_time']/max(1,summary['total_models'])/60:.1f}min\")\n",
    "\n",
    "print(f\"\\nüìà Success Rate:\")\n",
    "print(f\"   Total models: {summary['total_models']}\")\n",
    "print(f\"   Successful: {summary['successful']} ‚úÖ\")\n",
    "print(f\"   Failed: {summary['failed']} ‚ùå\")\n",
    "print(f\"   Success rate: {summary['success_rate']:.1f}%\")\n",
    "\n",
    "# Detailed results table\n",
    "if summary['results']:\n",
    "    print(f\"\\nüìã Detailed Results:\")\n",
    "    results_df = pd.DataFrame(summary['results'])\n",
    "    \n",
    "    # Format for display\n",
    "    display_cols = ['config', 'model_type', 'success', 'training_time', 'test_rmse', 'test_r2']\n",
    "    display_df = results_df[display_cols].copy() if all(col in results_df.columns for col in display_cols) else results_df\n",
    "    \n",
    "    if 'training_time' in display_df.columns:\n",
    "        display_df['training_time'] = display_df['training_time'].apply(lambda x: f\"{x/60:.1f}min\")\n",
    "    if 'test_rmse' in display_df.columns:\n",
    "        display_df['test_rmse'] = display_df['test_rmse'].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) else \"N/A\")\n",
    "    if 'test_r2' in display_df.columns:\n",
    "        display_df['test_r2'] = display_df['test_r2'].apply(lambda x: f\"{x:.4f}\" if pd.notna(x) else \"N/A\")\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "\n",
    "# Error summary\n",
    "if summary['errors']:\n",
    "    print(f\"\\n‚ùå Failed Models:\")\n",
    "    for error in summary['errors']:\n",
    "        print(f\"   - {error['config']} {error['model_type']}: {error['error']}\")\n",
    "\n",
    "# Save summary to file\n",
    "if SAVE_DETAILED_LOGS:\n",
    "    summary_file = f\"../models/training_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "    print(f\"\\nüíæ Summary saved to: {summary_file}\")\n",
    "\n",
    "print(f\"\\nüéâ All done! Check individual model folders in ../models/ for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307e035",
   "metadata": {},
   "source": [
    "## Quick Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d46b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä PERFORMANCE COMPARISON (12 models)\n",
      "================================================================================\n",
      " Config   Model                                                             Description     RMSE        R¬≤      MAE\n",
      " 30n_1n XGBoost                          30 days input to predict water level at day 31 0.212606  0.865615 0.124513\n",
      " 90n_7n XGBoost                          90 days input to predict water level at day 97 0.229131  0.839985 0.152529\n",
      " 30n_7n XGBoost 30 days input to predict water level at day 37 (not mean of days 31-37) 0.253469  0.809011 0.146621\n",
      "  7n_1n XGBoost    7 days input to predict water level at day 8 (not mean of days 8-14) 0.284123  0.759842 0.180669\n",
      "90n_30n XGBoost                         90 days input to predict water level at day 120 0.307339  0.710566 0.233413\n",
      " 90n_7n    LSTM                          90 days input to predict water level at day 97 0.351054  0.624388 0.213215\n",
      "30n_30n XGBoost                          30 days input to predict water level at day 60 0.369226  0.593873 0.271261\n",
      " 30n_7n    LSTM 30 days input to predict water level at day 37 (not mean of days 31-37) 0.511024  0.223677 0.321680\n",
      "90n_30n    LSTM                         90 days input to predict water level at day 120 0.517683  0.178812 0.348969\n",
      "  7n_1n    LSTM    7 days input to predict water level at day 8 (not mean of days 8-14) 0.545056  0.116173 0.342985\n",
      "30n_30n    LSTM                          30 days input to predict water level at day 60 0.554130  0.085253 0.386661\n",
      " 30n_1n    LSTM                          30 days input to predict water level at day 31 0.590098 -0.035262 0.361566\n",
      "\n",
      "üí° Quick Insights:\n",
      "   üèÜ Best overall: 30n_1n XGBoost (R¬≤ = 0.8656148451451403)\n",
      "   üìä XGBoost avg R¬≤: 0.763 | LSTM avg R¬≤: 0.199\n",
      "\n",
      "üíæ Comparison saved to: ../models/all_models_comparison.csv\n",
      "\n",
      "üéØ Next steps:\n",
      "   1. Run notebook 06_model_comparison.ipynb for detailed analysis\n",
      "   2. Check individual model folders for detailed results\n",
      "   3. Consider ensemble methods for best performing models\n",
      "   4. Deploy best model for production use\n"
     ]
    }
   ],
   "source": [
    "def load_all_model_results():\n",
    "    \"\"\"Load k·∫øt qu·∫£ t·ª´ t·∫•t c·∫£ models ƒë√£ train\"\"\"\n",
    "    models_folder = '../models'\n",
    "    all_results = []\n",
    "    \n",
    "    for config in EXPERIMENTS.keys():\n",
    "        # XGBoost results\n",
    "        xgb_results_file = f\"{models_folder}/{config}_xgb/results.json\"\n",
    "        if os.path.exists(xgb_results_file):\n",
    "            try:\n",
    "                with open(xgb_results_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                results['config'] = config\n",
    "                results['model_type'] = 'XGBoost'\n",
    "                results['description'] = EXPERIMENTS[config]['description']\n",
    "                all_results.append(results)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # LSTM results\n",
    "        lstm_results_file = f\"{models_folder}/{config}_lstm/results.json\"\n",
    "        if os.path.exists(lstm_results_file):\n",
    "            try:\n",
    "                with open(lstm_results_file, 'r') as f:\n",
    "                    results = json.load(f)\n",
    "                results['config'] = config\n",
    "                results['model_type'] = 'LSTM'\n",
    "                results['description'] = EXPERIMENTS[config]['description']\n",
    "                all_results.append(results)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Load v√† hi·ªÉn th·ªã comparison\n",
    "all_results = load_all_model_results()\n",
    "\n",
    "if all_results:\n",
    "    print(f\"\\nüìä PERFORMANCE COMPARISON ({len(all_results)} models)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for result in all_results:\n",
    "        row = {\n",
    "            'Config': result['config'],\n",
    "            'Model': result['model_type'],\n",
    "            'Description': result['description'],\n",
    "            'RMSE': result.get('test_metrics', {}).get('RMSE', 'N/A'),\n",
    "            'R¬≤': result.get('test_metrics', {}).get('R2', 'N/A'),\n",
    "            'MAE': result.get('test_metrics', {}).get('MAE', 'N/A')\n",
    "        }\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Sort by R¬≤ descending\n",
    "    if 'R¬≤' in comparison_df.columns:\n",
    "        comparison_df = comparison_df.sort_values('R¬≤', ascending=False)\n",
    "    \n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Quick insights\n",
    "    print(f\"\\nüí° Quick Insights:\")\n",
    "    \n",
    "    # Best model overall\n",
    "    numeric_df = comparison_df.copy()\n",
    "    numeric_df = numeric_df[numeric_df['R¬≤'] != 'N/A']\n",
    "    if not numeric_df.empty:\n",
    "        best_model = numeric_df.loc[numeric_df['R¬≤'].astype(float).idxmax()]\n",
    "        print(f\"   üèÜ Best overall: {best_model['Config']} {best_model['Model']} (R¬≤ = {best_model['R¬≤']})\")\n",
    "    \n",
    "    # Model type comparison\n",
    "    xgb_models = comparison_df[comparison_df['Model'] == 'XGBoost']\n",
    "    lstm_models = comparison_df[comparison_df['Model'] == 'LSTM']\n",
    "    \n",
    "    if not xgb_models.empty and not lstm_models.empty:\n",
    "        xgb_avg_r2 = xgb_models[xgb_models['R¬≤'] != 'N/A']['R¬≤'].astype(float).mean() if not xgb_models[xgb_models['R¬≤'] != 'N/A'].empty else 0\n",
    "        lstm_avg_r2 = lstm_models[lstm_models['R¬≤'] != 'N/A']['R¬≤'].astype(float).mean() if not lstm_models[lstm_models['R¬≤'] != 'N/A'].empty else 0\n",
    "        print(f\"   üìä XGBoost avg R¬≤: {xgb_avg_r2:.3f} | LSTM avg R¬≤: {lstm_avg_r2:.3f}\")\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_file = '../models/all_models_comparison.csv'\n",
    "    comparison_df.to_csv(comparison_file, index=False)\n",
    "    print(f\"\\nüíæ Comparison saved to: {comparison_file}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No model results found for comparison\")\n",
    "    print(f\"   Please train some models first\")\n",
    "\n",
    "print(f\"\\nüéØ Next steps:\")\n",
    "print(f\"   1. Run notebook 06_model_comparison.ipynb for detailed analysis\")\n",
    "print(f\"   2. Check individual model folders for detailed results\")\n",
    "print(f\"   3. Consider ensemble methods for best performing models\")\n",
    "print(f\"   4. Deploy best model for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67490655",
   "metadata": {},
   "source": [
    "## K·∫øt lu·∫≠n\n",
    "\n",
    "Notebook n√†y ƒë√£ ho√†n th√†nh vi·ªác training t·∫•t c·∫£ c√°c experiments cho c·∫£ XGBoost v√† LSTM models.\n",
    "\n",
    "### C√°c file ƒë∆∞·ª£c t·∫°o:\n",
    "- `../models/{config}_{model_type}/`: Th∆∞ m·ª•c ch·ª©a model v√† k·∫øt qu·∫£ chi ti·∫øt\n",
    "- `../models/all_models_comparison.csv`: B·∫£ng so s√°nh t·∫•t c·∫£ models\n",
    "- `../models/training_summary_{timestamp}.json`: Log chi ti·∫øt qu√° tr√¨nh training\n",
    "\n",
    "### T√≠nh nƒÉng ch√≠nh:\n",
    "- ‚úÖ T·ª± ƒë·ªông training tu·∫ßn t·ª± t·∫•t c·∫£ configurations\n",
    "- ‚úÖ Skip models ƒë√£ train (c√≥ th·ªÉ force retrain)\n",
    "- ‚úÖ Error handling v√† recovery\n",
    "- ‚úÖ Progress tracking v√† time estimation\n",
    "- ‚úÖ Comprehensive results summary\n",
    "- ‚úÖ Quick performance comparison\n",
    "\n",
    "### S·ª≠ d·ª•ng ti·∫øp:\n",
    "1. Ch·∫°y `06_model_comparison.ipynb` ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt\n",
    "2. Ch·ªçn model t·ªët nh·∫•t cho production\n",
    "3. Xem x√©t ensemble methods\n",
    "4. Monitor performance tr√™n data m·ªõi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdac887",
   "metadata": {},
   "source": [
    "## ‚úÖ Fixes Applied to Notebooks 03 & 04\n",
    "\n",
    "### D·ª±a tr√™n kinh nghi·ªám t·ª´ notebook 05, ƒë√£ fix c√°c l·ªói sau:\n",
    "\n",
    "#### **Notebook 03 (XGBoost Dynamic):**\n",
    "‚úÖ **Config fix**: `365n_7n, 365n_30n` ‚Üí `90n_7n, 90n_30n`  \n",
    "‚úÖ **Enhanced imports**: Th√™m datetime, seaborn  \n",
    "‚úÖ **Better validation**: Function-based data validation nh∆∞ notebook 05  \n",
    "‚úÖ **Progress tracking**: Th√™m timestamps v√† progress messages  \n",
    "‚úÖ **Error handling**: Try-catch v·ªõi detailed error messages  \n",
    "‚úÖ **Troubleshooting**: Th√™m section troubleshooting  \n",
    "\n",
    "#### **Notebook 04 (LSTM Dynamic):**\n",
    "‚úÖ **Config fix**: `365n_7n, 365n_30n` ‚Üí `90n_7n, 90n_30n`  \n",
    "‚úÖ **TensorFlow check**: Enhanced availability check nh∆∞ notebook 05  \n",
    "‚úÖ **Overfitting warnings**: Th√™m c·∫£nh b√°o v·ªÅ overfitting risk  \n",
    "‚úÖ **Data size check**: Warning khi dataset nh·ªè (<1000 samples)  \n",
    "‚úÖ **Enhanced training**: Epochs/patience handling nh∆∞ notebook 05  \n",
    "‚úÖ **Better error handling**: Improved try-catch blocks  \n",
    "\n",
    "#### **Improvements Applied:**\n",
    "- üîß Consistent configuration management\n",
    "- üìä Better progress tracking v√† time estimation  \n",
    "- ‚ö†Ô∏è Enhanced error handling v√† validation\n",
    "- üéØ Specific warnings for LSTM overfitting issues\n",
    "- üí° Troubleshooting sections added\n",
    "- üîÑ Code consistency with notebook 05 best practices\n",
    "\n",
    "#### **Expected Results:**\n",
    "- Notebooks 03 & 04 should now run reliably\n",
    "- Better error messages and validation\n",
    "- Consistent behavior with notebook 05\n",
    "- Warnings about LSTM performance issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waterlevel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
