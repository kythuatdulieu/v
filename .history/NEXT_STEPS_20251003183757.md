# Next Steps to Complete Fix #1

## âœ… Completed Edits

### Cell #3 (Feature Definition)
- âœ… `feature_cols_lstm = feature_cols_xgb` (both use 6 features)

### Cell #7 (Unified Scaling) - **JUST EDITED**
- âœ… Removed separate `scaler_lstm` and `scaler_xgb`
- âœ… Created single `scaler` that fits on training data only
- âœ… Created single `train_scaled` and `test_scaled` DataFrames
- âœ… Both contain all 6 features (water level + rainfall)
- âœ… Saved scaler to `../models/scalers/feature_scaler.pkl` (unified)
- âœ… Added clear warnings about target scaling

### Cell #9 (Function Definitions) - **JUST EDITED**
- âœ… Updated `create_sequences_lstm_daily` docstring
- âœ… Changed "CHá»ˆ WATER LEVEL" â†’ "CÃ™NG features vá»›i XGBoost"
- âœ… Added print showing features being used
- âœ… Clarified both functions use gap forecasting (single value at N+M)

### Cell #11 (Data Creation Loop) - **JUST EDITED**  
- âœ… Changed from `train_scaled_lstm` â†’ `train_scaled`
- âœ… Changed from `test_scaled_lstm` â†’ `test_scaled`
- âœ… Updated all print messages to say "SAME features as XGBoost"
- âœ… Updated metadata message to clarify feature parity

## ğŸ”„ Required Actions

### 1. Re-run Notebook 02 (CRITICAL)
```bash
# The notebook MUST be re-executed to regenerate data files
# Current data files still have 3 features for LSTM
```

**Execution order:**
1. Run Cell #1 (imports)
2. Run Cell #2 (constants and EXPERIMENTS)
3. Run Cell #3 (feature definitions) â† Verify 6 features printed
4. Run Cell #5 (old functions - can skip if not used)
5. Run Cell #7 (unified scaling) â† Verify single scaler created
6. Run Cell #8 (experiments config print)
7. Run Cell #9 (updated functions) â† Verify correct docstrings
8. Run Cell #11 (data creation loop) â† **THIS IS THE CRITICAL ONE**

**Expected output from Cell #11:**
```
Creating LSTM daily sequences: 7 days â†’ predict day 8
Features (6): ['Can Tho_water_level', 'Can Tho_rainfall', 'Chau Doc_water_level', ...]
Generated X sequences with shape (samples, 7, 6)
  â””â”€ samples Ã— 7 timesteps Ã— 6 features
```

**Current (wrong) output:**
```
Generated X sequences with shape (samples, 7, 3)  â† ONLY 3 FEATURES!
```

### 2. Verify Data Files
After re-running, check that data files are correct:

```bash
# For each experiment (7n_1n, 30n_1n, etc.)
python3 << 'EOF'
import numpy as np
import json

configs = ['7n_1n', '30n_1n', '30n_7n', '30n_30n', '90n_7n', '90n_30n']

for config in configs:
    lstm_path = f'../data/{config}_lstm/'
    
    # Load LSTM data
    X_train = np.load(f'{lstm_path}X_train.npy')
    
    # Load metadata
    with open(f'{lstm_path}metadata.json', 'r') as f:
        meta = json.load(f)
    
    # Check shape
    expected_features = 6
    actual_features = X_train.shape[2]
    
    status = "âœ…" if actual_features == expected_features else "âŒ"
    print(f"{status} {config}: LSTM shape = {X_train.shape} "
          f"(expected {expected_features} features, got {actual_features})")
    
    # Check metadata
    print(f"   Metadata: {meta['feature_info']}")
    print()
EOF
```

**Expected output:**
```
âœ… 7n_1n: LSTM shape = (samples, 7, 6) (expected 6 features, got 6)
   Metadata: SAME AS XGB: 6 variables (WL + Rainfall)

âœ… 30n_1n: LSTM shape = (samples, 30, 6) (expected 6 features, got 6)
   Metadata: SAME AS XGB: 6 variables (WL + Rainfall)
...
```

### 3. Re-train LSTM Models
Once data is regenerated with correct features:

```bash
# Open notebook 05_train_all_models.ipynb
# Run all cells to retrain LSTM models with correct 6 features
```

**Expected improvements:**
- Current RÂ²: 0.0 to 0.3 (very poor)
- After fix RÂ²: 0.5 to 0.7 (comparable to XGBoost)

### 4. Compare Models
```bash
# Open notebook 06_model_comparison.ipynb  
# Run all cells to compare XGBoost vs LSTM fairly
```

**Expected results:**
- Both models should have similar performance (Â±10%)
- LSTM should NO LONGER be handicapped by missing features

## ğŸ“Š Verification Checklist

Before re-running:
- [ ] Cell #7 creates single `scaler` (not `scaler_xgb` and `scaler_lstm`)
- [ ] Cell #7 creates single `train_scaled` (not `train_scaled_xgb` and `train_scaled_lstm`)
- [ ] Cell #11 uses `train_scaled` for both XGBoost and LSTM
- [ ] Cell #9 docstring says "CÃ™NG features vá»›i XGBoost"

After re-running Cell #11:
- [ ] LSTM data shape is (samples, N, 6) not (samples, N, 3)
- [ ] Metadata shows "SAME AS XGB: 6 variables"
- [ ] All 6 config folders created successfully

After re-training (notebook 05):
- [ ] LSTM RÂ² improved significantly (from ~0.0-0.3 to 0.5-0.7)
- [ ] LSTM and XGBoost have comparable performance

## ğŸ” Key Changes Summary

### Before Fix:
```python
# Cell #7 (OLD - WRONG)
scaler_xgb = StandardScaler()
scaler_lstm = StandardScaler()
train_scaled_xgb = scaler_xgb.fit_transform(train_data[feature_cols_xgb])  # 6 features
train_scaled_lstm = scaler_lstm.fit_transform(train_data[feature_cols_lstm])  # 3 features!

# Cell #11 (OLD - WRONG)
create_sequences_lstm_daily(train_scaled_lstm, ...)  # Only 3 features!
```

### After Fix:
```python
# Cell #7 (NEW - CORRECT)
scaler = StandardScaler()
train_scaled = scaler.fit_transform(train_data[feature_cols_xgb])  # 6 features
# Single DataFrame used by BOTH models

# Cell #11 (NEW - CORRECT)
create_sequences_lstm_daily(train_scaled, ...)  # All 6 features!
```

## âš ï¸ Critical Notes

1. **Must re-run notebook** - Code changes alone are not enough. The data files must be regenerated.

2. **Check kernel variables** - After re-running, verify:
   ```python
   print(train_scaled.shape)  # Should show all 6 features
   print(feature_cols_lstm)   # Should match feature_cols_xgb
   ```

3. **Sequential execution** - Do NOT run cells out of order. Start from Cell #1 and go sequentially.

4. **Disk space** - Each config creates ~2 folders with numpy arrays. Ensure sufficient space.

5. **Time estimate** - Re-running Cell #11 will take ~30-60 seconds per config (6 configs total).

## ğŸ“ User Instructions

Äá»ƒ hoÃ n táº¥t fix:

1. **Má»Ÿ notebook** `02_feature_engineering.ipynb`
2. **Restart kernel** Ä‘á»ƒ clear old variables
3. **Run All Cells** tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i
4. **Verify output** cá»§a Cell #11:
   - LSTM shape pháº£i lÃ  (samples, N, **6**) khÃ´ng pháº£i (samples, N, 3)
5. **Kiá»ƒm tra files** trong `../data/`:
   - Má»—i `*_lstm/metadata.json` pháº£i ghi "SAME AS XGB: 6 variables"
6. **Re-train LSTM** báº±ng notebook 05
7. **So sÃ¡nh káº¿t quáº£** báº±ng notebook 06

## ğŸ¯ Success Criteria

Fix Ä‘Æ°á»£c coi lÃ  hoÃ n táº¥t khi:
- âœ… LSTM input shape: (samples, N, 6) cho táº¥t cáº£ configs
- âœ… Metadata files confirm 6 features
- âœ… LSTM RÂ² >= 0.5 (comparable to XGBoost)
- âœ… KhÃ´ng cÃ²n performance gap lá»›n giá»¯a LSTM vÃ  XGBoost
