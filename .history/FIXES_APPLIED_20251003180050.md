# âœ… TÃ³m táº¯t cÃ¡c fixes Ä‘Ã£ hoÃ n thÃ nh

**NgÃ y:** 3 ThÃ¡ng 10, 2025  
**Tráº¡ng thÃ¡i:** âœ… ÄÃƒ FIX 3 Váº¤N Äá»€ QUAN TRá»ŒNG NHáº¤T

---

## ğŸ¯ CÃ¡c fixes Ä‘Ã£ hoÃ n thÃ nh

### âœ… Fix #1: ThÃªm Rainfall Features cho LSTM (CRITICAL - 70% impact)

**File Ä‘Ã£ sá»­a:** `notebooks/02_feature_engineering.ipynb`

**Thay Ä‘á»•i:**
```python
# TRÆ¯á»šC (SAI):
feature_cols_lstm = [col for col in train_data.columns if 'Water Level' in col]
# â†’ LSTM chá»‰ cÃ³ 3 features (3 water level stations)

# SAU (ÄÃšNG):
feature_cols_lstm = [col for col in train_data.columns if col not in ['datetime', 'month'] 
                     and 'WL_Change' not in col]
# â†’ LSTM cÃ³ 6 features (3 water level + 3 rainfall) - GIá»NG XGBoost
```

**TÃ¡c Ä‘á»™ng:**
- LSTM giá» cÃ³ Ä‘á»§ thÃ´ng tin Ä‘á»ƒ dá»± Ä‘oÃ¡n (rainfall lÃ  crucial predictor)
- Expected improvement: RÂ² tá»« Ã¢m/0.3 â†’ 0.6-0.7
- Fair comparison vá»›i XGBoost

---

### âœ… Fix #2: Sequential Validation Split (HIGH - 15% impact)

**File Ä‘Ã£ sá»­a:** `src/lstm_trainer.py`

**Thay Ä‘á»•i trong method `grid_search()` vÃ  `train_best_model()`:**
```python
# TRÆ¯á»šC (SAI):
history = model.fit(
    self.X_train, self.y_train,
    validation_split=0.2,  # âŒ Random split â†’ temporal leakage
    ...
)

# SAU (ÄÃšNG):
val_samples = int(len(self.X_train) * 0.2)
train_samples = len(self.X_train) - val_samples

X_train_fold = self.X_train[:train_samples]
y_train_fold = self.y_train[:train_samples]
X_val_fold = self.X_train[train_samples:]
y_val_fold = self.y_train[train_samples:]

history = model.fit(
    X_train_fold, y_train_fold,
    validation_data=(X_val_fold, y_val_fold),  # âœ… Sequential split
    ...
)
```

**TÃ¡c Ä‘á»™ng:**
- KhÃ´ng cÃ²n temporal leakage trong validation
- Hyperparameter tuning Ä‘Ã¡ng tin cáº­y hÆ¡n
- Model generalize tá»‘t hÆ¡n

---

### âœ… Fix #3: Scale Target (y) cho LSTM (MEDIUM - 10% impact)

**File Ä‘Ã£ sá»­a:** `src/lstm_trainer.py`

**Thay Ä‘á»•i:**

1. **ThÃªm target_scaler vÃ o `__init__`:**
```python
self.target_scaler = None  # Will be fitted in load_data
```

2. **Scale target trong `load_data()`:**
```python
from sklearn.preprocessing import StandardScaler

# Load raw data
y_train_raw = np.load(f"{folder}/y_train.npy")
y_test_raw = np.load(f"{folder}/y_test.npy")

# Scale target
self.target_scaler = StandardScaler()
self.y_train = self.target_scaler.fit_transform(y_train_raw.reshape(-1, 1)).flatten()
self.y_test = self.target_scaler.transform(y_test_raw.reshape(-1, 1)).flatten()

# LÆ°u y gá»‘c Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
self.y_train_original = y_train_raw
self.y_test_original = y_test_raw
```

3. **Inverse transform trong `evaluate()`:**
```python
# Predictions (scaled)
y_train_pred_scaled = self.model.predict(self.X_train, verbose=0).squeeze()
y_test_pred_scaled = self.model.predict(self.X_test, verbose=0).squeeze()

# Inverse transform vá» thang Ä‘o gá»‘c
y_train_pred = self.target_scaler.inverse_transform(
    y_train_pred_scaled.reshape(-1, 1)
).flatten()
y_test_pred = self.target_scaler.inverse_transform(
    y_test_pred_scaled.reshape(-1, 1)
).flatten()

# Metrics trÃªn thang Ä‘o Gá»C
train_metrics = {
    'MAE': mean_absolute_error(self.y_train_original, y_train_pred),
    'RMSE': np.sqrt(mean_squared_error(self.y_train_original, y_train_pred)),
    'R2': r2_score(self.y_train_original, y_train_pred)
}
```

4. **Save scaler trong `save_results()`:**
```python
import joblib
joblib.dump(self.target_scaler, f"{config_folder}/target_scaler.pkl")
```

**TÃ¡c Ä‘á»™ng:**
- LSTM há»c cÃ¢n báº±ng hÆ¡n (features vÃ  target Ä‘á»u scaled)
- Training á»•n Ä‘á»‹nh hÆ¡n, converge nhanh hÆ¡n
- Metrics Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn thang Ä‘o gá»‘c (dá»… interpret)

---

## ğŸ“‹ CÃ¡c bÆ°á»›c tiáº¿p theo

### BÆ°á»›c 1: Re-run Feature Engineering (Báº®T BUá»˜C)

Notebook `02_feature_engineering.ipynb` Ä‘Ã£ Ä‘Æ°á»£c sá»­a, bÃ¢y giá» cáº§n cháº¡y láº¡i Ä‘á»ƒ táº¡o dá»¯ liá»‡u má»›i:

```bash
# KÃ­ch hoáº¡t virtual environment
cd /home/duclinh/v
source .venv/bin/activate

# Cháº¡y notebook (tá»« terminal hoáº·c Jupyter)
jupyter nbconvert --to notebook --execute notebooks/02_feature_engineering.ipynb
```

**Hoáº·c má»Ÿ notebook trong VS Code vÃ  cháº¡y tá»«ng cell.**

**Quan trá»ng:** 
- Pháº£i cháº¡y láº¡i notebook 02 vÃ¬ `feature_cols_lstm` Ä‘Ã£ thay Ä‘á»•i
- Dá»¯ liá»‡u LSTM má»›i sáº½ cÃ³ 6 features thay vÃ¬ 3
- Sáº½ táº¡o láº¡i files trong `data/*_lstm/`

---

### BÆ°á»›c 2: Re-train LSTM Models

Sau khi cÃ³ dá»¯ liá»‡u má»›i, cháº¡y notebook `05_train_all_models.ipynb` Ä‘á»ƒ train láº¡i LSTM:

```bash
jupyter nbconvert --to notebook --execute notebooks/05_train_all_models.ipynb
```

**Hoáº·c má»Ÿ notebook vÃ  cháº¡y pháº§n LSTM.**

**Expected results:**
- LSTM RÂ² sáº½ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ
- Training sáº½ á»•n Ä‘á»‹nh hÆ¡n (nhá» sequential validation vÃ  target scaling)
- CÃ³ thá»ƒ cáº¡nh tranh vá»›i XGBoost

---

### BÆ°á»›c 3: Compare Results

So sÃ¡nh káº¿t quáº£ trÆ°á»›c vÃ  sau:

| Metric | LSTM (Before) | LSTM (After Fix) | XGBoost | Improvement |
|--------|---------------|------------------|---------|-------------|
| Test MAE | ? | ? | ? | ? |
| Test RMSE | ? | ? | ? | ? |
| Test RÂ² | ~0.0-0.3 | Expected: 0.6-0.7 | ~0.6-0.7 | +100-300% |
| Features | 3 (only WL) | 6 (WL + Rainfall) | 6 | +100% |

---

## ğŸ” Kiá»ƒm tra fixes Ä‘Ã£ apply Ä‘Ãºng chÆ°a

### Check Fix #1 (Rainfall features):
```bash
grep -n "feature_cols_lstm" /home/duclinh/v/notebooks/02_feature_engineering.ipynb
```
NÃªn tháº¥y: `feature_cols_lstm` dÃ¹ng cÃ¹ng filter vá»›i `feature_cols_xgb`

### Check Fix #2 (Sequential validation):
```bash
grep -A 5 "validation_data" /home/duclinh/v/src/lstm_trainer.py
```
NÃªn tháº¥y: `validation_data=(X_val_fold, y_val_fold)` thay vÃ¬ `validation_split=`

### Check Fix #3 (Target scaling):
```bash
grep -n "target_scaler" /home/duclinh/v/src/lstm_trainer.py | head -10
```
NÃªn tháº¥y: 
- Line ~49: `self.target_scaler = None`
- Line ~67: `self.target_scaler = StandardScaler()`
- Line ~280: `inverse_transform`
- Line ~378: `joblib.dump(self.target_scaler...)`

---

## âš ï¸ LÆ°u Ã½ quan trá»ng

### 1. Virtual Environment
Äáº£m báº£o Ä‘Ã£ activate virtual environment trÆ°á»›c khi cháº¡y:
```bash
source .venv/bin/activate
```

### 2. Dependencies
Kiá»ƒm tra cÃ³ Ä‘á»§ packages:
```bash
pip list | grep -E "(tensorflow|sklearn|pandas|numpy|joblib)"
```

Náº¿u thiáº¿u, install:
```bash
pip install tensorflow scikit-learn pandas numpy joblib
```

### 3. Data Backup
TrÆ°á»›c khi re-run feature engineering, backup dá»¯ liá»‡u cÅ©:
```bash
mkdir -p data_backup
cp -r data/*_lstm data_backup/
```

### 4. Computational Resources
- LSTM training cÃ³ thá»ƒ máº¥t vÃ i giá»
- Náº¿u cÃ³ GPU, TensorFlow sáº½ tá»± Ä‘á»™ng dÃ¹ng
- Check GPU: `python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"`

---

## ğŸ“Š Expected Timeline

| Task | Duration | Notes |
|------|----------|-------|
| Re-run notebook 02 | 10-30 mins | Depending on data size |
| Re-train LSTM (1 config) | 30-60 mins | With grid search |
| Re-train all configs (6 configs) | 3-6 hours | Can run overnight |
| Analysis & comparison | 30 mins | Using notebook 06 |

**Total:** ~4-7 hours (cÃ³ thá»ƒ Ä‘á»ƒ cháº¡y overnight)

---

## ğŸ¯ Success Criteria

Sau khi hoÃ n thÃ nh táº¥t cáº£ fixes vÃ  re-training, báº¡n sáº½ tháº¥y:

âœ… **LSTM Test RÂ² > 0.5** (hiá»‡n táº¡i: ~0.0-0.3)  
âœ… **LSTM comparable vá»›i XGBoost** (Â±10%)  
âœ… **Training curves á»•n Ä‘á»‹nh** (khÃ´ng oscillate máº¡nh)  
âœ… **Validation loss giáº£m smoothly** (khÃ´ng flat line)  
âœ… **No more negative RÂ²** (regression nightmare)

---

## ğŸ› Troubleshooting

### Náº¿u gáº·p lá»—i "ModuleNotFoundError":
```bash
pip install <missing_module>
```

### Náº¿u notebook 02 cháº¡y lá»—i:
- Check data files tá»“n táº¡i: `ls data/train_data.csv data/test_data.csv`
- Check config: `python -c "from config import EXPERIMENTS; print(EXPERIMENTS)"`

### Náº¿u LSTM váº«n perform kÃ©m:
- Check data shape: Pháº£i lÃ  `(samples, timesteps, 6)` chá»© khÃ´ng pháº£i `(samples, timesteps, 3)`
- Check scaler: `print(trainer.target_scaler)` pháº£i khÃ´ng None
- Check validation: Console pháº£i show "validation_data" chá»© khÃ´ng pháº£i "validation_split"

### Náº¿u cáº§n rollback:
```bash
git checkout src/lstm_trainer.py
git checkout notebooks/02_feature_engineering.ipynb
```

---

## ğŸ“ Next Steps

1. âœ… Äá»c document nÃ y
2. â–¶ï¸ **Cháº¡y notebook 02** (feature engineering)
3. â–¶ï¸ **Cháº¡y notebook 05** (training)
4. ğŸ“Š **Cháº¡y notebook 06** (comparison)
5. ğŸ‰ **Celebrate improvements!**

---

**Good luck! Expected improvement: LSTM RÂ² from ~0.2 to ~0.6 (3x better!)**

---

**Táº¡o bá»Ÿi:** GitHub Copilot  
**NgÃ y:** 3 ThÃ¡ng 10, 2025  
**PhiÃªn báº£n:** 1.0 - Post-Fix Summary
