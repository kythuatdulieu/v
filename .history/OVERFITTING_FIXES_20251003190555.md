# ğŸ”´ LSTM Overfitting - Giáº£i phÃ¡p

## Váº¥n Ä‘á» hiá»‡n táº¡i
- Train RÂ²: **0.911** (ráº¥t cao)
- Test RÂ²: **0.297** (ráº¥t tháº¥p)
- Gap: **-68%** (overfitting nghiÃªm trá»ng)
- XGBoost Test RÂ²: **0.760** (cao hÆ¡n LSTM 2.5 láº§n!)

## So sÃ¡nh vá»›i XGBoost
```
XGBoost: Train 0.979 â†’ Test 0.760 (gap -22%, cháº¥p nháº­n Ä‘Æ°á»£c)
LSTM:    Train 0.911 â†’ Test 0.297 (gap -68%, KHÃ”NG cháº¥p nháº­n Ä‘Æ°á»£c)
```

## NguyÃªn nhÃ¢n

### 1. Dropout khÃ´ng Ä‘á»§ máº¡nh
Hiá»‡n táº¡i: `dropout = 0.2` (chá»‰ táº¯t 20% neurons)
- LSTM cÃ³ 31,651 parameters (ráº¥t nhiá»u cho 7440 samples)
- Ratio: 0.24 samples/param â†’ dá»… overfit

### 2. Model quÃ¡ phá»©c táº¡p
```json
"best_params": {
    "units": 50,
    "n_layers": 2,  â† 2 layers cÃ³ thá»ƒ quÃ¡ nhiá»u
    "dropout": 0.2  â† quÃ¡ tháº¥p
}
```

### 3. Epochs quÃ¡ cao
- Trained 85 epochs
- CÃ³ thá»ƒ Ä‘Ã£ há»c "thuá»™c lÃ²ng" training patterns
- Early stopping vá»›i patience=10 chÆ°a Ä‘á»§ aggressive

### 4. KhÃ´ng cÃ³ Regularization
- KhÃ´ng cÃ³ L1/L2 regularization
- KhÃ´ng cÃ³ RecurrentDropout

## Giáº£i phÃ¡p

### Fix #1: TÄƒng Dropout (QUAN TRá»ŒNG NHáº¤T)
```python
# File: config.py
LSTM_PARAMS = {
    'units': [50, 100],
    'n_layers': [1, 2],
    'dropout': [0.3, 0.4, 0.5],  # â† TÄƒng tá»« [0.1, 0.2] lÃªn [0.3, 0.4, 0.5]
    'batch_size': [256],
    'epochs': [100],
    'patience': [10]
}
```

### Fix #2: ThÃªm RecurrentDropout
```python
# File: src/lstm_trainer.py, trong build_model()

# Thay vÃ¬:
model.add(LSTM(units, return_sequences=True, dropout=dropout))

# Sá»­a thÃ nh:
model.add(LSTM(
    units, 
    return_sequences=True, 
    dropout=dropout,
    recurrent_dropout=0.2  # â† THÃŠM recurrent_dropout
))
```

### Fix #3: Giáº£m patience cá»§a Early Stopping
```python
# File: config.py
LSTM_PARAMS = {
    ...
    'patience': [5, 7]  # â† Giáº£m tá»« [10] xuá»‘ng [5, 7]
}
```

### Fix #4: ThÃªm L2 Regularization
```python
# File: src/lstm_trainer.py
from tensorflow.keras.regularizers import l2

# Trong build_model():
model.add(LSTM(
    units,
    return_sequences=True,
    dropout=dropout,
    recurrent_dropout=0.2,
    kernel_regularizer=l2(0.01)  # â† THÃŠM L2 regularization
))
```

### Fix #5: Batch Normalization
```python
# File: src/lstm_trainer.py
from tensorflow.keras.layers import BatchNormalization

# Sau má»—i LSTM layer:
model.add(LSTM(...))
model.add(BatchNormalization())  # â† THÃŠM batch normalization
model.add(Dropout(dropout))
```

## Æ¯u tiÃªn thá»±c hiá»‡n

### NGAY Láº¬P Tá»¨C (High Impact, Low Effort):
1. âœ… **Fix #1: TÄƒng dropout** - Sá»­a config.py, re-train
   - Expected: Test RÂ² tÄƒng tá»« 0.297 â†’ 0.4-0.5

2. âœ… **Fix #3: Giáº£m patience** - Sá»­a config.py, re-train
   - Prevent overfitting sá»›m hÆ¡n

### SAU ÄÃ“ (Medium Impact, Medium Effort):
3. **Fix #2: Recurrent dropout** - Sá»­a lstm_trainer.py
   - Regularize hidden states

4. **Fix #4: L2 regularization** - Sá»­a lstm_trainer.py
   - Penalize large weights

### Náº¾U VáºªN CHÆ¯A Tá»T (High Impact, High Effort):
5. **Fix #5: Batch normalization** - Sá»­a lstm_trainer.py
   - Stabilize training

## Káº¿t quáº£ mong Ä‘á»£i

### Sau Fix #1 + #3:
- Test RÂ²: **0.4 - 0.5** (tÄƒng tá»« 0.297)
- Train-Test gap: **< 40%** (giáº£m tá»« 68%)

### Sau táº¥t cáº£ fixes:
- Test RÂ²: **0.5 - 0.65** (so vá»›i XGBoost 0.76)
- Train-Test gap: **< 30%**
- LSTM competitive vá»›i XGBoost

## Lá»‡nh thá»±c hiá»‡n

```bash
# 1. Sá»­a config.py
# 2. Re-train LSTM
cd /home/duclinh/v
# Open notebook 05_train_all_models.ipynb
# Run all cells

# 3. So sÃ¡nh káº¿t quáº£
# Open notebook 06_model_comparison.ipynb
# Run all cells
```

## Kiá»ƒm tra thÃªm

### Data Leakage Check:
```python
# Verify sequential split trong lstm_trainer.py
# Äáº£m báº£o validation data LUÃ”N SAU training data
```

### Feature Correlation:
```python
# Check multicollinearity giá»¯a cÃ¡c features
# CÃ³ thá»ƒ cáº§n feature selection náº¿u correlation > 0.9
```

### Temporal Patterns:
```python
# Visualize predictions vs actuals
# Check xem LSTM cÃ³ há»c Ä‘Æ°á»£c seasonal patterns khÃ´ng
```

## Debug Training Process

ThÃªm logging vÃ o lstm_trainer.py:
```python
# Sau má»—i epoch in ra:
print(f"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}")
```

Kiá»ƒm tra:
- Val_loss cÃ³ giáº£m Ä‘á»u khÃ´ng?
- CÃ³ Ä‘iá»ƒm nÃ o val_loss báº¯t Ä‘áº§u tÄƒng trong khi train_loss giáº£m? (overfitting point)
